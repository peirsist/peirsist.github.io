<!DOCTYPE html>
<html>
	<head>
		
<title>Intro_Math.md-Quiet</title>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<link rel="shortcut icon" type="image/x-icon" href="/image/favicon.ico">

<link rel="stylesheet" href="/css/index.css">


<meta name="keywords" content="hexo主题,hexo扁平化主题,Quiet主题">
<meta name="description" content="描述">


<script src="/js/jquery.min.js"></script>


<script src="/js/index.js"></script>


<script src="/js/fancybox.umd.js"></script>


<script src="/js/fancybox-images.js"></script>


<script src="/js/gitalk.min.js"></script>


<script src="/js/hljs.min.js"></script>
 
<script>hljs.highlightAll();</script>

	<meta name="generator" content="Hexo 6.3.0"></head>

	<body>
		
	<div class="header">
		<div class="header-top" id="header-top">
			<div class="h-left">
				<a href="/">
					<img src="/image/logo.png" alt="Quiet">
				</a>
			</div>
			<div class="h-right">
				<ul>
					
						
								<li>
									<a href="/">
										HOME
									</a>
									<span class="dot"></span>
								</li>
								
									
						
								<li>
									<a href="/archives">
										ARCHIVE
									</a>
									<span class="dot"></span>
								</li>
								
									
						
								<li>
									<a href="/categories">
										CATEGORIES
									</a>
									<span class="dot"></span>
								</li>
								
									
						
								<li>
									<a href="/tags">
										TAGS
									</a>
									<span class="dot"></span>
								</li>
								
									
						
								<li>
									<a href="/links">
										LINKS
									</a>
									<span class="dot"></span>
								</li>
								
									
						
								<li>
									<a href="/about">
										ABOUT
									</a>
									<span class="dot"></span>
								</li>
								
									
				</ul>
			</div>
			<div class="h-right-close">
				<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="24" height="24">
					<path fill="none" d="M0 0h24v24H0z" />
					<path d="M3 4h18v2H3V4zm0 7h18v2H3v-2zm0 7h18v2H3v-2z" fill="rgba(68,68,68,1)" />
				</svg>
			</div>
		</div>
	</div>
	<div class="sidebar">
    <div class="topo">
        <h2>Joey</h2>
    </div>
    <ul>
        
        <li>
            <a href="/">HOME</a>
        </li>
        
        <li>
            <a href="/archives">ARCHIVE</a>
        </li>
        
        <li>
            <a href="/categories">CATEGORIES</a>
        </li>
        
        <li>
            <a href="/tags">TAGS</a>
        </li>
        
        <li>
            <a href="/links">LINKS</a>
        </li>
        
        <li>
            <a href="/about">ABOUT</a>
        </li>
        
    </ul>
    <div class="my_foot">
        
        <a target="_blank" rel="noopener" href="https://github.com/79E/hexo-theme-quiet">
            <img src="https://cdn.jsdelivr.net/gh/duogongneng/MyBlogImg/imggithub.png" alt="Quiet主题">
        </a>
        
    </div>
</div>
<div class='shelter'>
</div>
<style>
    .shelter{
        background-color: #333;
        opacity:0.5;
        cursor: pointer;
        display: none; 
        position: fixed;
        left: 0;
        top: 0; 
        right: 0;
        bottom: 0;
        z-index: 1998;
    }
    .sidebar {
        width: 66%;
        height: 100%;
        position: fixed;
        top: 0;
        right: -100%;
        bottom: 0;
        background: #fff;
        z-index: 1999;
        text-align: center;
        box-shadow: -6px 0 20px rgba(98, 94, 94, .815);
    }

    .topo {
        width: 100%;
        height: 200px;
        background: url(https://api.ixiaowai.cn/gqapi/gqapi.php) no-repeat;
        background-size: 100% 100%;
        position: relative;
        display: flex;
        align-items: flex-end
    }

    .topo h2 {
        color: #fff;
        z-index: 1;
        position: relative;
        margin: 0 0 10px 10px;
        font-size: 1.2em;
        box-sizing: border-box
    }

    .topo:before {
        content: '';
        background-image: url(/image/pattern.png);
        background-repeat: repeat;
        height: 100%;
        left: 0;
        position: absolute;
        top: 0;
        width: 100%;
        z-index: 1
    }

    .sidebar ul {
        width: 100%;
        margin-top: 50px
    }

    .sidebar ul li {
        height: 50px;
        list-style: none;
        font-size: 1.2em;
        text-align: right;
        margin-right: 10px
    }

    .sidebar ul li a {
        display: grid;
        color: #5d606a;
        text-overflow: ellipsis;
        width: 100%;
        text-decoration: none
    }

    .my_foot {
        width: 100%;
        padding: 10px;
        margin-bottom: 10px;
        position: absolute;
        bottom: 0
    }

    .my_foot a {
        text-decoration: none;
        margin-right: 10px;
        display: inline-block
    }

    .my_foot a img {
        width: 30px;
        height: 30px
    }
</style>

<script>
    $( function () {
	$( '.h-right-close>svg' )
		.click( function () {
			$( '.sidebar' )
				.animate( {
					right: "0"
				}, 500 );
			$( '.shelter' )
				.fadeIn( "slow" )
		} );
	$( '.shelter' )
		.click( function ( e ) {
			$( '.sidebar' )
				.animate( {
					right: "-100%"
				}, 500 );
			$( '.shelter' )
				.fadeOut( "slow" )
		} )
} )

</script>

<div class="post">
    <div class="post-header-background post-header-img"
    style="background: url('https://api.ixiaowai.cn/gqapi/gqapi.php')" 
>
    <div class="post-header-background-content">
        <ul class="post-header-tag">
            
        </ul>
        
        <h1>Intro_Math.md</h1>
        <div class="post-header-info">
            <div class="post-header-info-author">
                
                    <svg t="1604839279282" class="icon" viewBox="0 0 1024 1024" version="1.1"
                        xmlns="http://www.w3.org/2000/svg" p-id="2901" width="20" height="20">
                        <path
                            d="M513 956.3c-247.7 0-448-200.3-448-448S265.3 66.2 513 66.2s448 200.3 448 448-200.3 442.1-448 442.1z m0-830.9c-212.2 0-388.8 170.7-388.8 388.8C124.2 726.3 294.9 903 513 903c212.2 0 388.8-170.7 388.8-388.8S725.2 125.4 513 125.4z m0 430.2c-94.2 0-170.7-76.5-170.7-170.7S418.8 207.8 513 207.8s170.7 76.5 170.7 170.7S607.2 555.6 513 555.6z m0-289.1c-64.6 0-112 52.8-112 112s47.4 117.9 112 117.9 112-52.8 112-112-47.4-117.9-112-117.9z m0 689.8c-135.7 0-259-58.7-341.9-158.9l-11.8-17.8 11.8-17.8c76.5-117.9 206.2-188.5 347.8-188.5 135.7 0 265 64.6 341.9 182.6l11.8 17.8-11.8 17.8C778 897.1 648.7 956.3 513 956.3zM230.3 773.2C300.9 849.7 406.9 897 513 897c112 0 218.1-47.4 288.6-129.8-70.5-88.2-170.7-135.6-282.7-135.6s-218.1 53.3-288.6 141.6z"
                            p-id="2902" fill="#ffffff"></path>
                    </svg>
                    
                <span class="post-header-info-author-text"> <a href="../../about">Joey</a></span>
                <div class="post-header-info-author-categories">
                    
                         <a href="../../categories/机器学习/" target="_blank" >机器学习</a>
                    
                </div>
                <p>2022-10-20 21:44:41</p>
            </div>
        </div>
    </div>
</div>
    <div class="post-content" id="content">
  
  <div id="article" class="post-content-info">
    

    <h1 id="introduction">Introduction</h1>
<p>对概率的诠释有两大学派，一种是频率派另一种是贝叶斯派。后面我们对观测集采用下面记号： <span class="math display">\[
X_{N\times p}=(x_{1},x_{2},\cdots,x_{N})^{T},x_{i}=(x_{i1},x_{i2},\cdots,x_{ip})^{T}
\]</span> 这个记号表示有 <span class="math inline">\(N\)</span> 个样本，每个样本都是 <span class="math inline">\(p\)</span> 维向量。其中每个观测都是由 <span class="math inline">\(p(x|\theta)\)</span> 生成的。</p>
<h2 id="频率派的观点">频率派的观点</h2>
<p><span class="math inline">\(p(x|\theta)\)</span>中的 <span class="math inline">\(\theta\)</span> 是一个常量。对于 <span class="math inline">\(N\)</span> 个观测来说观测集的概率为 <span class="math inline">\(p(X|\theta)\mathop{=}\limits _{iid}\prod\limits _{i=1}^{N}p(x_{i}|\theta))\)</span> 。为了求 <span class="math inline">\(\theta\)</span> 的大小，我们采用最大对数似然MLE的方法：</p>
<p><span class="math display">\[
\theta_{MLE}=\mathop{argmax}\limits _{\theta}\log p(X|\theta)\mathop{=}\limits _{iid}\mathop{argmax}\limits _{\theta}\sum\limits _{i=1}^{N}\log p(x_{i}|\theta)
\]</span></p>
<h2 id="贝叶斯派的观点">贝叶斯派的观点</h2>
<p>贝叶斯派认为 <span class="math inline">\(p(x|\theta)\)</span> 中的 <span class="math inline">\(\theta\)</span> 不是一个常量。这个 <span class="math inline">\(\theta\)</span> 满足一个预设的先验的分布 <span class="math inline">\(\theta\sim p(\theta)\)</span> 。于是根据贝叶斯定理依赖观测集参数的后验可以写成：</p>
<p><span class="math display">\[
p(\theta|X)=\frac{p(X|\theta)\cdot p(\theta)}{p(X)}=\frac{p(X|\theta)\cdot p(\theta)}{\int\limits _{\theta}p(X|\theta)\cdot p(\theta)d\theta}
\]</span> 为了求 <span class="math inline">\(\theta\)</span> 的值，我们要最大化这个参数后验MAP：</p>
<p><span class="math display">\[
\theta_{MAP}=\mathop{argmax}\limits _{\theta}p(\theta|X)=\mathop{argmax}\limits _{\theta}p(X|\theta)\cdot p(\theta)
\]</span> 其中第二个等号是由于分母和 <span class="math inline">\(\theta\)</span> 没有关系。求解这个 <span class="math inline">\(\theta\)</span> 值后计算<span class="math inline">\(\frac{p(X|\theta)\cdot p(\theta)}{\int\limits _{\theta}p(X|\theta)\cdot p(\theta)d\theta}\)</span> ，就得到了参数的后验概率。其中 <span class="math inline">\(p(X|\theta)\)</span> 叫似然，是我们的模型分布。得到了参数的后验分布后，我们可以将这个分布用于预测贝叶斯预测： <span class="math display">\[
p(x_{new}|X)=\int\limits _{\theta}p(x_{new}|\theta)\cdot p(\theta|X)d\theta
\]</span> 其中积分中的被乘数是模型，乘数是后验分布。</p>
<h2 id="小结">小结</h2>
<p>频率派和贝叶斯派分别给出了一系列的机器学习算法。频率派的观点导出了一系列的统计机器学习算法而贝叶斯派导出了概率图理论。在应用频率派的 MLE 方法时最优化理论占有重要地位。而贝叶斯派的算法无论是后验概率的建模还是应用这个后验进行推断时积分占有重要地位。因此采样积分方法如 MCMC 有很多应用。</p>
<h1 id="mathbasics">MathBasics</h1>
<h2 id="高斯分布">高斯分布</h2>
<h3 id="一维情况-mle">一维情况 MLE</h3>
<p>高斯分布在机器学习中占有举足轻重的作用。在 MLE 方法中：</p>
<p><span class="math display">\[
\theta=(\mu,\Sigma)=(\mu,\sigma^{2}),\theta_{MLE}=\mathop{argmax}\limits _{\theta}\log p(X|\theta)\mathop{=}\limits _{iid}\mathop{argmax}\limits _{\theta}\sum\limits _{i=1}^{N}\log p(x_{i}|\theta)
\]</span> 一般地，高斯分布的概率密度函数PDF写为：</p>
<p><span class="math display">\[
p(x|\mu,\Sigma)=\frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}}e^{-\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)}
\]</span> 带入 MLE 中我们考虑一维的情况</p>
<p><span class="math display">\[
\log p(X|\theta)=\sum\limits _{i=1}^{N}\log p(x_{i}|\theta)=\sum\limits _{i=1}^{N}\log\frac{1}{\sqrt{2\pi}\sigma}\exp(-(x_{i}-\mu)^{2}/2\sigma^{2})
\]</span> 首先对 <span class="math inline">\(\mu\)</span> 的极值可以得到 ： <span class="math display">\[
\mu_{MLE}=\mathop{argmax}\limits _{\mu}\log p(X|\theta)=\mathop{argmax}\limits _{\mu}\sum\limits _{i=1}^{N}(x_{i}-\mu)^{2}
\]</span> 于是： <span class="math display">\[
\frac{\partial}{\partial\mu}\sum\limits _{i=1}^{N}(x_{i}-\mu)^{2}=0\longrightarrow\mu_{MLE}=\frac{1}{N}\sum\limits _{i=1}^{N}x_{i}
\]</span> 其次对 <span class="math inline">\(\theta\)</span> 中的另一个参数 <span class="math inline">\(\sigma\)</span> ，有： <span class="math display">\[
\begin{align}
\sigma_{MLE}=\mathop{argmax}\limits _{\sigma}\log p(X|\theta)&amp;=\mathop{argmax}\limits _{\sigma}\sum\limits _{i=1}^{N}[-\log\sigma-\frac{1}{2\sigma^{2}}(x_{i}-\mu)^{2}]\nonumber\\
&amp;=\mathop{argmin}\limits _{\sigma}\sum\limits _{i=1}^{N}[\log\sigma+\frac{1}{2\sigma^{2}}(x_{i}-\mu)^{2}]
\end{align}
\]</span> 于是： <span class="math display">\[
\frac{\partial}{\partial\sigma}\sum\limits _{i=1}^{N}[\log\sigma+\frac{1}{2\sigma^{2}}(x_{i}-\mu)^{2}]=0\longrightarrow\sigma_{MLE}^{2}=\frac{1}{N}\sum\limits _{i=1}^{N}(x_{i}-\mu)^{2}
\]</span> 值得注意的是，上面的推导中，首先对 <span class="math inline">\(\mu\)</span> 求 MLE， 然后利用这个结果求 <span class="math inline">\(\sigma_{MLE}\)</span> ，因此可以预期的是对数据集求期望时 <span class="math inline">\(\mathbb{E}_{\mathcal{D}}[\mu_{MLE}]\)</span> 是无偏差的： <span class="math display">\[
\mathbb{E}_{\mathcal{D}}[\mu_{MLE}]=\mathbb{E}_{\mathcal{D}}[\frac{1}{N}\sum\limits _{i=1}^{N}x_{i}]=\frac{1}{N}\sum\limits _{i=1}^{N}\mathbb{E}_{\mathcal{D}}[x_{i}]=\mu
\]</span> 但是当对 <span class="math inline">\(\sigma_{MLE}\)</span> 求 期望的时候由于使用了单个数据集的 <span class="math inline">\(\mu_{MLE}\)</span>，因此对所有数据集求期望的时候我们会发现 <span class="math inline">\(\sigma_{MLE}\)</span> 是 有偏的：</p>
<p><span class="math display">\[
\begin{align}
\mathbb{E}_{\mathcal{D}}[\sigma_{MLE}^{2}]&amp;=\mathbb{E}_{\mathcal{D}}[\frac{1}{N}\sum\limits _{i=1}^{N}(x_{i}-\mu_{MLE})^{2}]=\mathbb{E}_{\mathcal{D}}[\frac{1}{N}\sum\limits _{i=1}^{N}(x_{i}^{2}-2x_{i}\mu_{MLE}+\mu_{MLE}^{2})\nonumber
\\&amp;=\mathbb{E}_{\mathcal{D}}[\frac{1}{N}\sum\limits _{i=1}^{N}x_{i}^{2}-\mu_{MLE}^{2}]=\mathbb{E}_{\mathcal{D}}[\frac{1}{N}\sum\limits _{i=1}^{N}x_{i}^{2}-\mu^{2}+\mu^{2}-\mu_{MLE}^{2}]\nonumber\\
&amp;= \mathbb{E}_{\mathcal{D}}[\frac{1}{N}\sum\limits _{i=1}^{N}x_{i}^{2}-\mu^{2}]-\mathbb{E}_{\mathcal{D}}[\mu_{MLE}^{2}-\mu^{2}]=\sigma^{2}-(\mathbb{E}_{\mathcal{D}}[\mu_{MLE}^{2}]-\mu^{2})\nonumber\\&amp;=\sigma^{2}-(\mathbb{E}_{\mathcal{D}}[\mu_{MLE}^{2}]-\mathbb{E}_{\mathcal{D}}^{2}[\mu_{MLE}])=\sigma^{2}-Var[\mu_{MLE}]\nonumber\\&amp;=\sigma^{2}-Var[\frac{1}{N}\sum\limits _{i=1}^{N}x_{i}]=\sigma^{2}-\frac{1}{N^{2}}\sum\limits _{i=1}^{N}Var[x_{i}]=\frac{N-1}{N}\sigma^{2}
\end{align}
\]</span> 所以： <span class="math display">\[
\hat{\sigma}^{2}=\frac{1}{N-1}\sum\limits _{i=1}^{N}(x_{i}-\mu)^{2}
\]</span></p>
<h3 id="多维情况">多维情况</h3>
<p>多维高斯分布表达式为： <span class="math display">\[
p(x|\mu,\Sigma)=\frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}}e^{-\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)}
\]</span> 其中 <span class="math inline">\(x,\mu\in\mathbb{R}^{p},\Sigma\in\mathbb{R}^{p\times p}\)</span> ，<span class="math inline">\(\Sigma\)</span> 为协方差矩阵，一般而言也是半正定矩阵。这里我们只考虑正定矩阵。首先我们处理指数上的数字，指数上的数字可以记为 <span class="math inline">\(x\)</span> 和 <span class="math inline">\(\mu\)</span> 之间的马氏距离。对于对称的协方差矩阵可进行特征值分解，<span class="math inline">\(\Sigma=U\Lambda U^{T}=(u_{1},u_{2},\cdots,u_{p})diag(\lambda_{i})(u_{1},u_{2},\cdots,u_{p})^{T}=\sum\limits _{i=1}^{p}u_{i}\lambda_{i}u_{i}^{T}\)</span> ，于是：</p>
<p><span class="math display">\[
\Sigma^{-1}=\sum\limits _{i=1}^{p}u_{i}\frac{1}{\lambda_{i}}u_{i}^{T}
\]</span></p>
<p><span class="math display">\[
\Delta=(x-\mu)^{T}\Sigma^{-1}(x-\mu)=\sum\limits _{i=1}^{p}(x-\mu)^{T}u_{i}\frac{1}{\lambda_{i}}u_{i}^{T}(x-\mu)=\sum\limits _{i=1}^{p}\frac{y_{i}^{2}}{\lambda_{i}}
\]</span></p>
<p>我们注意到 <span class="math inline">\(y_{i}\)</span> 是 <span class="math inline">\(x-\mu\)</span> 在特征向量 <span class="math inline">\(u_{i}\)</span> 上的投影长度，因此上式子就是 <span class="math inline">\(\Delta\)</span> 取不同值时的同心椭圆。</p>
<p>下面我们看多维高斯模型在实际应用时的两个问题</p>
<ol type="1">
<li><p>参数 <span class="math inline">\(\Sigma,\mu\)</span> 的自由度为 <span class="math inline">\(O(p^{2})\)</span> 对于维度很高的数据其自由度太高。解决方案：高自由度的来源是 <span class="math inline">\(\Sigma\)</span> 有 <span class="math inline">\(\frac{p(p+1)}{2}\)</span> 个自由参数，可以假设其是对角矩阵，甚至在各向同性假设中假设其对角线上的元素都相同。前一种的算法有 Factor Analysis，后一种有概率 PCA(p-PCA) 。</p></li>
<li><p>第二个问题是单个高斯分布是单峰的，对有多个峰的数据分布不能得到好的结果。解决方案：高斯混合GMM 模型。</p></li>
</ol>
<p>下面对多维高斯分布的常用定理进行介绍。</p>
<p>我们记 <span class="math inline">\(x=(x_1, x_2,\cdots,x_p)^T=(x_{a,m\times 1}, x_{b,n\times1})^T,\mu=(\mu_{a,m\times1}, \mu_{b,n\times1}),\Sigma=\begin{pmatrix}\Sigma_{aa}&amp;\Sigma_{ab}\\\Sigma_{ba}&amp;\Sigma_{bb}\end{pmatrix}\)</span>，已知 <span class="math inline">\(x\sim\mathcal{N}(\mu,\Sigma)\)</span>。</p>
<p>首先是一个高斯分布的定理：</p>
<blockquote>
<p>定理：已知 <span class="math inline">\(x\sim\mathcal{N}(\mu,\Sigma), y\sim Ax+b\)</span>，那么 <span class="math inline">\(y\sim\mathcal{N}(A\mu+b, A\Sigma A^T)\)</span>。</p>
<p>证明：<span class="math inline">\(\mathbb{E}[y]=\mathbb{E}[Ax+b]=A\mathbb{E}[x]+b=A\mu+b\)</span>，<span class="math inline">\(Var[y]=Var[Ax+b]=Var[Ax]=A\cdot Var[x]\cdot A^T\)</span>。</p>
</blockquote>
<p>下面利用这个定理得到 <span class="math inline">\(p(x_a),p(x_b),p(x_a|x_b),p(x_b|x_a)\)</span> 这四个量。</p>
<ol type="1">
<li><p><span class="math inline">\(x_a=\begin{pmatrix}\mathbb{I}_{m\times m}&amp;\mathbb{O}_{m\times n})\end{pmatrix}\begin{pmatrix}x_a\\x_b\end{pmatrix}\)</span>，代入定理中得到： <span class="math display">\[
\mathbb{E}[x_a]=\begin{pmatrix}\mathbb{I}&amp;\mathbb{O}\end{pmatrix}\begin{pmatrix}\mu_a\\\mu_b\end{pmatrix}=\mu_a\\
Var[x_a]=\begin{pmatrix}\mathbb{I}&amp;\mathbb{O}\end{pmatrix}\begin{pmatrix}\Sigma_{aa}&amp;\Sigma_{ab}\\\Sigma_{ba}&amp;\Sigma_{bb}\end{pmatrix}\begin{pmatrix}\mathbb{I}\\\mathbb{O}\end{pmatrix}=\Sigma_{aa}
\]</span> 所以 <span class="math inline">\(x_a\sim\mathcal{N}(\mu_a,\Sigma_{aa})\)</span>。</p></li>
<li><p>同样的，<span class="math inline">\(x_b\sim\mathcal{N}(\mu_b,\Sigma_{bb})\)</span>。</p></li>
<li><p>对于两个条件概率，我们引入三个量： <span class="math display">\[
x_{b\cdot a}=x_b-\Sigma_{ba}\Sigma_{aa}^{-1}x_a\\
\mu_{b\cdot a}=\mu_b-\Sigma_{ba}\Sigma_{aa}^{-1}\mu_a\\
\Sigma_{bb\cdot a}=\Sigma_{bb}-\Sigma_{ba}\Sigma_{aa}^{-1}\Sigma_{ab}
\]</span> 特别的，最后一个式子叫做 <span class="math inline">\(\Sigma_{bb}\)</span> 的 Schur Complementary。可以看到： <span class="math display">\[
x_{b\cdot a}=\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&amp;\mathbb{I}_{n\times n}\end{pmatrix}\begin{pmatrix}x_a\\x_b\end{pmatrix}
\]</span> 所以： <span class="math display">\[
\mathbb{E}[x_{b\cdot a}]=\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&amp;\mathbb{I}_{n\times n}\end{pmatrix}\begin{pmatrix}\mu_a\\\mu_b\end{pmatrix}=\mu_{b\cdot a}\\
Var[x_{b\cdot a}]=\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&amp;\mathbb{I}_{n\times n}\end{pmatrix}\begin{pmatrix}\Sigma_{aa}&amp;\Sigma_{ab}\\\Sigma_{ba}&amp;\Sigma_{bb}\end{pmatrix}\begin{pmatrix}-\Sigma_{aa}^{-1}\Sigma_{ba}^T\\\mathbb{I}_{n\times n}\end{pmatrix}=\Sigma_{bb\cdot a}
\]</span> 利用这三个量可以得到 <span class="math inline">\(x_b=x_{b\cdot a}+\Sigma_{ba}\Sigma_{aa}^{-1}x_a\)</span>。因此： <span class="math display">\[
\mathbb{E}[x_b|x_a]=\mu_{b\cdot a}+\Sigma_{ba}\Sigma_{aa}^{-1}x_a
\]</span></p>
<p><span class="math display">\[
Var[x_b|x_a]=\Sigma_{bb\cdot a}
\]</span></p>
<p>这里同样用到了定理。</p></li>
<li><p>同样： <span class="math display">\[
x_{a\cdot b}=x_a-\Sigma_{ab}\Sigma_{bb}^{-1}x_b\\
\mu_{a\cdot b}=\mu_a-\Sigma_{ab}\Sigma_{bb}^{-1}\mu_b\\
\Sigma_{aa\cdot b}=\Sigma_{aa}-\Sigma_{ab}\Sigma_{bb}^{-1}\Sigma_{ba}
\]</span> 所以： <span class="math display">\[
\mathbb{E}[x_a|x_b]=\mu_{a\cdot b}+\Sigma_{ab}\Sigma_{bb}^{-1}x_b
\]</span></p>
<p><span class="math display">\[
Var[x_a|x_b]=\Sigma_{aa\cdot b}
\]</span></p></li>
</ol>
<p>下面利用上边四个量，求解线性模型：</p>
<blockquote>
<p>已知：<span class="math inline">\(p(x)=\mathcal{N}(\mu,\Lambda^{-1}),p(y|x)=\mathcal{N}(Ax+b,L^{-1})\)</span>，求解：<span class="math inline">\(p(y),p(x|y)\)</span>。</p>
<p>解：令 <span class="math inline">\(y=Ax+b+\epsilon,\epsilon\sim\mathcal{N}(0,L^{-1})\)</span>，所以 <span class="math inline">\(\mathbb{E}[y]=\mathbb{E}[Ax+b+\epsilon]=A\mu+b\)</span>，<span class="math inline">\(Var[y]=A \Lambda^{-1}A^T+L^{-1}\)</span>，因此： <span class="math display">\[
  p(y)=\mathcal{N}(A\mu+b,L^{-1}+A\Lambda^{-1}A^T)
  \]</span> 引入 <span class="math inline">\(z=\begin{pmatrix}x\\y\end{pmatrix}\)</span>，我们可以得到 <span class="math inline">\(Cov[x,y]=\mathbb{E}[(x-\mathbb{E}[x])(y-\mathbb{E}[y])^T]\)</span>。对于这个协方差可以直接计算： <span class="math display">\[
  \begin{align}
  Cov(x,y)&amp;=\mathbb{E}[(x-\mu)(Ax-A\mu+\epsilon)^T]=\mathbb{E}[(x-\mu)(x-\mu)^TA^T]=Var[x]A^T=\Lambda^{-1}A^T
  \end{align}
  \]</span> 注意到协方差矩阵的对称性，所以 <span class="math inline">\(p(z)=\mathcal{N}\begin{pmatrix}\mu\\A\mu+b\end{pmatrix},\begin{pmatrix}\Lambda^{-1}&amp;\Lambda^{-1}A^T\\A\Lambda^{-1}&amp;L^{-1}+A\Lambda^{-1}A^T\end{pmatrix})\)</span>。根据之前的公式，我们可以得到： <span class="math display">\[
  \mathbb{E}[x|y]=\mu+\Lambda^{-1}A^T(L^{-1}+A\Lambda^{-1}A^T)^{-1}(y-A\mu-b)
  \]</span></p>
<p><span class="math display">\[
  Var[x|y]=\Lambda^{-1}-\Lambda^{-1}A^T(L^{-1}+A\Lambda^{-1}A^T)^{-1}A\Lambda^{-1}
  \]</span></p>
</blockquote>

  </div>
  <div id="gitalk-container"></div>
</div>

<script>
  
Fancybox.bind('[data-fancybox="fancybox-gallery-img"]', {
  dragToClose: true,
  Toolbar: true,
  closeButton: "top",
  Image: {
    zoom: true,
  },
  on: {
    initCarousel: (fancybox) => {
      const slide = fancybox.Carousel.slides[fancybox.Carousel.page];
      fancybox.$container.style.setProperty(
        "--bg-image",
        `url("${slide.$thumb.src}")`
      );
    },
    "Carousel.change": (fancybox, carousel, to, from) => {
      const slide = carousel.slides[to];
      fancybox.$container.style.setProperty(
        "--bg-image",
        `url("${slide.$thumb.src}")`
      );
    },
  },
});
</script>

<style>
    #noneimg img {
        display: none;
        z-index: 9999;
        /* width: 600px !important; */
        min-width: 0%;
        max-width: 90%;
        max-height: 80%;
        border-radius: 0px;
        position: fixed;
        box-shadow: 0 0 0px #c3c3c300 !important;
        left: 0;
        top: 0;
        right: 0;
        bottom: 0;
        margin: auto !important;
    }

    @media screen and (max-width:600px) {
        #noneimg img {
            max-width: 88%
        }
    }
</style>

    <div class="post-paging">
    
    <a href="/2022/10/20/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2.LinearRegression/">
        <div class="post-paging-last">
            <span>上一篇</span>
            <p>线性回归.md</p>
        </div>
    </a>
    

    
    <a href="/2022/10/20/test-md/">
        <div class="post-paging-next">
            <span>下一篇</span>
            <p>test.md</p>
        </div>
    </a>
    
</div>
</div>
		
<div class="footer">
	<div class="Copyright">
		©2023 By Joey. 主题：<a
			style="text-decoration: none;display: contents; color: #898F9F;"
			target="_blank" rel="noopener" href="https://github.com/79e/hexo-theme-quiet">Quiet</a>
	</div>
	<div class="contact">
		
		<a target="_blank" rel="noopener" href="https://github.com/79E/hexo-theme-quiet">
			<img src="https://cdn.jsdelivr.net/gh/duogongneng/MyBlogImg/imggithub.png" alt="Quiet主题">
		</a>
		
	</div>
</div>

<script src="/js/gotop.js"></script>


<style type="text/css">
    @media screen and (min-width: 600px) {
        .goTop>span {
            display: flex;
            align-items: center;
            justify-content: center;
            border-radius: 10px;
            width: 40px;
            height: 40px;
            cursor: pointer;
            opacity: 0.8;
            background: rgba(18, 24, 58, 0.06);
            text-align: center;
            transition: border .5s;
            border: 1px solid rgba(18, 24, 58, 0.06);

            -moz-transition: border .5s;
            /* Firefox 4 */
            -webkit-transition: border .5s;
            /* Safari 和 Chrome */
            -o-transition: border .5s;
            /* Opera */
        }

        .goTop>span:hover {
            border: 1px solid #6680B3;
        }


        .goTop {
            position: fixed;
            right: 30px;
            bottom: 80px;
        }

        .goTop>span>svg {
            width: 20px;
            height: 20px;
            opacity: 0.7;
        }

    }

    @media screen and (max-width: 600px) {
        .goTop {
            display: none;
        }
    }
</style>
<div class="goTop" id="js-go_top">
    <span>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24">
            <g>
                <path d="M13 12v8h-2v-8H4l8-8 8 8z"></path>
            </g>
        </svg>
    </span>
</div>
<script>
    $( '#js-go_top' )
	.gotoTop( {
		offset: 500,
		speed: 300,
		animationShow: {
			'transform': 'translate(0,0)',
			'transition': 'transform .5s ease-in-out'
		},
		animationHide: {
			'transform': 'translate(100px,0)',
			'transition': 'transform .5s ease-in-out'
		}
	} );
</script>


    <!-- Gitalk -->
    <script>
        const data = '{"clientID":"02b3c","clientSecret":"adfc7b4","repo":"gimment","owner":"duneng","admin":"duneng"}'
        const gitalk = new Gitalk({
            ...JSON.parse( data),
            id:location.pathname,
            distractionFreeMode:false
        })
        
        if(Boolean('true')){
            gitalk.render('gitalk-container')
        }
    </script>

<script>
	console.log('\n %c Hexo-Quiet 主题 %c https://github.com/79e/hexo-theme-quiet \n', 'color: #fadfa3; background: #030307; padding:5px 0;', 'background: #fadfa3; padding:5px 0;')
</script>
	</body>
</html>

