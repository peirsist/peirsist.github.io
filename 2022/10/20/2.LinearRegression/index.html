<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <meta name="google-site-verification" content="" />
  <meta name="referrer" content="unsafe-url">
  
  <title>线性回归.md</title>
  <meta name="author" content="Peirsist">
  <meta name="description" content="线性回归
假设数据集为： \[
\mathcal{D}=\{(x_1, y_1),(x_2, y_2),\cdots,(x_N, y_N)\}
\] 后面我们记： \[
X=(x_1,x_2,\cdots,x_N)^T,Y=(y_1,y_2,\cdots,y_N)^T
\] 线性回归假设：">
  
  
  <meta property="og:title" content="线性回归.md"/>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:site_name" content="EEWIKI"/>
  <link href="/apple-touch-icon-precomposed.png" sizes="180x180" rel="apple-touch-icon-precomposed">
  <link rel="alternate" href="/atom.xml" title="EEWIKI" type="application/atom+xml">
  <link rel="stylesheet" href="/css/m.min.css">
  <link rel="icon" type="image/x-icon" href="/favicon.ico">
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script>

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <a id="top"></a>
  <div id="main">
    <div class="main-ctnr">
      <div class="behind">
  <a href="/" class="back black-color">
    <svg class="i-close" viewBox="0 0 32 32" width="22" height="22" fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="3">
        <path d="M2 30 L30 2 M30 30 L2 2"></path>
    </svg>
  </a>
  
</div>


  <article class="standard post">
    <div class="title">
      
  
    <h1 class="page-title center">
        线性回归.md
    </h1>
  


    </div>
    <div class="meta center">
      <time datetime="2022-10-20T13:44:41.000Z" itemprop="datePublished">
  <svg class="i-calendar" viewBox="0 0 32 32" width="16" height="16" fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2">
    <path d="M2 6 L2 30 30 30 30 6 Z M2 15 L30 15 M7 3 L7 9 M13 3 L13 9 M19 3 L19 9 M25 3 L25 9"></path>
  </svg>
  &nbsp;
  2022-10-20
</time>


    
    &nbsp;
    <svg class="i-tag" viewBox="0 0 32 32" width="16" height="16" fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2">
      <circle cx="24" cy="8" r="2"></circle>
      <path d="M2 18 L18 2 30 2 30 14 14 30 Z"></path>
    </svg>
    &nbsp;
    <a href="/categories/机器学习/">机器学习</a>





    </div>
    <hr>
    
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-text">线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95"><span class="toc-text">最小二乘法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%99%AA%E5%A3%B0%E4%B8%BA%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83%E7%9A%84-mle"><span class="toc-text">噪声为高斯分布的 MLE</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9D%83%E9%87%8D%E5%85%88%E9%AA%8C%E4%B9%9F%E4%B8%BA%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83%E7%9A%84-map"><span class="toc-text">权重先验也为高斯分布的 MAP</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-text">正则化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#l1-lasso"><span class="toc-text">L1 Lasso</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#l2-ridge"><span class="toc-text">L2 Ridge</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%8F%E7%BB%93"><span class="toc-text">小结</span></a></li></ol></li></ol>
    
    <div class="picture-container">
      
    </div>
    <h1 id="线性回归">线性回归</h1>
<p>假设数据集为： <span class="math display">\[
\mathcal{D}=\{(x_1, y_1),(x_2, y_2),\cdots,(x_N, y_N)\}
\]</span> 后面我们记： <span class="math display">\[
X=(x_1,x_2,\cdots,x_N)^T,Y=(y_1,y_2,\cdots,y_N)^T
\]</span> 线性回归假设： <span class="math display">\[
f(w)=w^Tx
\]</span></p>
<h2 id="最小二乘法">最小二乘法</h2>
<p>对这个问题，采用二范数定义的平方误差来定义损失函数： <span class="math display">\[
L(w)=\sum\limits_{i=1}^N||w^Tx_i-y_i||^2_2
\]</span> 展开得到： <span class="math display">\[
\begin{align}
L(w)&amp;=(w^Tx_1-y_1,\cdots,w^Tx_N-y_N)\cdot (w^Tx_1-y_1,\cdots,w^Tx_N-y_N)^T\nonumber\\
&amp;=(w^TX^T-Y^T)\cdot (Xw-Y)=w^TX^TXw-Y^TXw-w^TX^TY+Y^TY\nonumber\\
&amp;=w^TX^TXw-2w^TX^TY+Y^TY
\end{align}
\]</span> 最小化这个值的 $ $ ： <span class="math display">\[
\begin{align}
\hat{w}=\mathop{argmin}\limits_wL(w)&amp;\longrightarrow\frac{\partial}{\partial w}L(w)=0\nonumber\\
&amp;\longrightarrow2X^TX\hat{w}-2X^TY=0\nonumber\\
&amp;\longrightarrow \hat{w}=(X^TX)^{-1}X^TY=X^+Y
\end{align}
\]</span> 这个式子中 <span class="math inline">\((X^TX)^{-1}X^T\)</span> 又被称为伪逆。对于行满秩或者列满秩的 <span class="math inline">\(X\)</span>，可以直接求解，但是对于非满秩的样本集合，需要使用奇异值分解（SVD）的方法，对 <span class="math inline">\(X\)</span> 求奇异值分解，得到 <span class="math display">\[
X=U\Sigma V^T
\]</span> 于是： <span class="math display">\[
X^+=V\Sigma^{-1}U^T
\]</span> 在几何上，最小二乘法相当于模型（这里就是直线）和试验值的距离的平方求和，假设我们的试验样本张成一个 <span class="math inline">\(p\)</span> 维空间（满秩的情况）：<span class="math inline">\(X=Span(x_1,\cdots,x_N)\)</span>，而模型可以写成 <span class="math inline">\(f(w)=X\beta\)</span>，也就是 <span class="math inline">\(x_1,\cdots,x_N\)</span> 的某种组合，而最小二乘法就是说希望 <span class="math inline">\(Y\)</span> 和这个模型距离越小越好，于是它们的差应该与这个张成的空间垂直： <span class="math display">\[
X^T\cdot(Y-X\beta)=0\longrightarrow\beta=(X^TX)^{-1}X^TY
\]</span></p>
<h2 id="噪声为高斯分布的-mle">噪声为高斯分布的 MLE</h2>
<p>对于一维的情况，记 <span class="math inline">\(y=w^Tx+\epsilon,\epsilon\sim\mathcal{N}(0,\sigma^2)\)</span>，那么 <span class="math inline">\(y\sim\mathcal{N}(w^Tx,\sigma^2)\)</span>。代入极大似然估计中： <span class="math display">\[
\begin{align}
L(w)=\log p(Y|X,w)&amp;=\log\prod\limits_{i=1}^Np(y_i|x_i,w)\nonumber\\
&amp;=\sum\limits_{i=1}^N\log(\frac{1}{\sqrt{2\pi\sigma}}e^{-\frac{(y_i-w^Tx_i)^2}{2\sigma^2}})\\
\mathop{argmax}\limits_wL(w)&amp;=\mathop{argmin}\limits_w\sum\limits_{i=1^N}(y_i-w^Tx_i)^2
\end{align}
\]</span> 这个表达式和最小二乘估计得到的结果一样。</p>
<h2 id="权重先验也为高斯分布的-map">权重先验也为高斯分布的 MAP</h2>
<p>取先验分布 <span class="math inline">\(w\sim\mathcal{N}(0,\sigma_0^2)\)</span>。于是：  <span class="math display">\[
\begin{align}
\hat{w}=\mathop{argmax}\limits_wp(w|Y)&amp;=\mathop{argmax}\limits_wp(Y|w)p(w)\nonumber\\
&amp;=\mathop{argmax}\limits_w\log p(Y|w)p(w)\nonumber\\
&amp;=\mathop{argmax}\limits_w(\log p(Y|w)+\log p(w))\nonumber\\
&amp;=\mathop{argmin}\limits_w[(y-w^Tx)^2+\frac{\sigma^2}{\sigma_0^2}w^Tw]
\end{align}
\]</span> 这里省略了 <span class="math inline">\(X\)</span>，<span class="math inline">\(p(Y)\)</span>和 <span class="math inline">\(w\)</span> 没有关系，同时也利用了上面高斯分布的 MLE的结果。</p>
<p>我们将会看到，超参数 <span class="math inline">\(\sigma_0\)</span>的存在和下面会介绍的 Ridge 正则项可以对应，同样的如果将先验分布取为 Laplace 分布，那么就会得到和 L1 正则类似的结果。</p>
<h2 id="正则化">正则化</h2>
<p>在实际应用时，如果样本容量不远远大于样本的特征维度，很可能造成过拟合，对这种情况，我们有下面三个解决方式：</p>
<ol type="1">
<li>加数据</li>
<li>特征选择（降低特征维度）如 PCA 算法。</li>
<li>正则化</li>
</ol>
<p>正则化一般是在损失函数（如上面介绍的最小二乘损失）上加入正则化项（表示模型的复杂度对模型的惩罚），下面我们介绍一般情况下的两种正则化框架。 <span class="math display">\[
\begin{align}
L1&amp;:\mathop{argmin}\limits_wL(w)+\lambda||w||_1,\lambda\gt0\\
L2&amp;:\mathop{argmin}\limits_wL(w)+\lambda||w||^2_2,\lambda \gt 0
\end{align}
\]</span> 下面对最小二乘误差分别分析这两者的区别。</p>
<h3 id="l1-lasso">L1 Lasso</h3>
<p>L1正则化可以引起稀疏解。</p>
<p>从最小化损失的角度看，由于 L1 项求导在0附近的左右导数都不是0，因此更容易取到0解。</p>
<p>从另一个方面看，L1 正则化相当于： <span class="math display">\[
\mathop{argmin}\limits_wL(w)\\
s.t. ||w||_1\lt C
\]</span> 我们已经看到平方误差损失函数在 <span class="math inline">\(w\)</span> 空间是一个椭球，因此上式求解就是椭球和 <span class="math inline">\(||w||_1=C\)</span>的切点，因此更容易相切在坐标轴上。</p>
<h3 id="l2-ridge">L2 Ridge</h3>
<p><span class="math display">\[
\begin{align}
\hat{w}=\mathop{argmin}\limits_wL(w)+\lambda w^Tw&amp;\longrightarrow\frac{\partial}{\partial w}L(w)+2\lambda w=0\nonumber\\
&amp;\longrightarrow2X^TX\hat{w}-2X^TY+2\lambda \hat w=0\nonumber\\
&amp;\longrightarrow \hat{w}=(X^TX+\lambda \mathbb{I})^{-1}X^TY
\end{align}
\]</span></p>
<p>可以看到，这个正则化参数和前面的 MAP 结果不谋而合。利用2范数进行正则化不仅可以是模型选择 <span class="math inline">\(w\)</span> 较小的参数，同时也避免 $ X^TX$不可逆的问题。</p>
<h2 id="小结">小结</h2>
<p>线性回归模型是最简单的模型，但是麻雀虽小，五脏俱全，在这里，我们利用最小二乘误差得到了闭式解。同时也发现，在噪声为高斯分布的时候，MLE 的解等价于最小二乘误差，而增加了正则项后，最小二乘误差加上 L2 正则项等价于高斯噪声先验下的 MAP解，加上 L1 正则项后，等价于 Laplace 噪声先验。</p>
<p>传统的机器学习方法或多或少都有线性回归模型的影子：</p>
<ol type="1">
<li>线性模型往往不能很好地拟合数据，因此有三种方案克服这一劣势：
<ol type="1">
<li>对特征的维数进行变换，例如多项式回归模型就是在线性特征的基础上加入高次项。</li>
<li>在线性方程后面加入一个非线性变换，即引入一个非线性的激活函数，典型的有线性分类模型如感知机。</li>
<li>对于一致的线性系数，我们进行多次变换，这样同一个特征不仅仅被单个系数影响，例如多层感知机（深度前馈网络）。</li>
</ol></li>
<li>线性回归在整个样本空间都是线性的，我们修改这个限制，在不同区域引入不同的线性或非线性，例如线性样条回归和决策树模型。</li>
<li>线性回归中使用了所有的样本，但是对数据预先进行加工学习的效果可能更好（所谓的维数灾难，高维度数据更难学习），例如 PCA 算法和流形学习。</li>
</ol>


  </article>
  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  <div class="busuanzi center">
    page PV:&nbsp;<span id="busuanzi_value_page_pv"></span>&nbsp;・&nbsp;
    site PV:&nbsp;<span id="busuanzi_value_site_pv"></span>&nbsp;・&nbsp;
    site UV:&nbsp;<span id="busuanzi_value_site_uv"></span>
  </div>


    





    </div>
  </div>
  <footer class="page-footer"><div class="clearfix">
</div>
<div class="right-foot">
    <div class="firstrow">
        <a href="#top" target="_self">
        <svg class="i-caret-right" viewBox="0 0 32 32" width="24" height="24" fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="3">
            <path d="M10 30 L26 16 10 2 Z"></path>
        </svg>
        </a>
        © peirsist 2022
    </div>
    <div class="secondrow">
        <a target="_blank" rel="noopener" href="https://github.com/gaoryrt/hexo-theme-pln">
        Theme Pln
        </a>
    </div>
</div>
<div class="clearfix">
</div>
</footer>
  <script src="//cdn.bootcss.com/jquery/2.2.1/jquery.min.js"></script>
<script src="/js/search.min.js"></script>
<script type="text/javascript">

// disqus scripts


// dropdown scripts
$(".dropdown").click(function(event) {
  var current = $(this);
  event.stopPropagation();
  $(current).children(".dropdown-content")[($(current).children(".dropdown-content").hasClass("open"))?'removeClass':'addClass']("open")
});
$(document).click(function(){
    $(".dropdown-content").removeClass("open");
})

var path = "/search.xml";
searchFunc(path, 'local-search-input', 'local-search-result');

</script>

</body>
</html>
