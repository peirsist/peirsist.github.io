<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <meta name="google-site-verification" content="" />
  <meta name="referrer" content="unsafe-url">
  
  <title>线性回归.md</title>
  <meta name="author" content="Peirsist">
  <meta name="description" content="线性回归假设数据集为：$$\mathcal{D}&amp;#x3D;{(x_1, y_1),(x_2, y_2),\cdots,(x_N, y_N)}$$后面我们记：$$X&amp;#x3D;(x_1,x_2,\cdots,x_N)^T,Y&amp;#x3D;(y_1,y_2,\cdots,y_N)^T$$线性回归假设：$">
  
  
  <meta property="og:title" content="线性回归.md"/>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:site_name" content="EEWIKI"/>
  <link href="/apple-touch-icon-precomposed.png" sizes="180x180" rel="apple-touch-icon-precomposed">
  <link rel="alternate" href="/atom.xml" title="EEWIKI" type="application/atom+xml">
  <link rel="stylesheet" href="/css/m.min.css">
  <link rel="icon" type="image/x-icon" href="/favicon.ico">
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script>

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <a id="top"></a>
  <div id="main">
    <div class="main-ctnr">
      <div class="behind">
  <a href="/" class="back black-color">
    <svg class="i-close" viewBox="0 0 32 32" width="22" height="22" fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="3">
        <path d="M2 30 L30 2 M30 30 L2 2"></path>
    </svg>
  </a>
  
</div>


  <article class="standard post">
    <div class="title">
      
  
    <h1 class="page-title center">
        线性回归.md
    </h1>
  


    </div>
    <div class="meta center">
      <time datetime="2022-10-20T13:44:41.000Z" itemprop="datePublished">
  <svg class="i-calendar" viewBox="0 0 32 32" width="16" height="16" fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2">
    <path d="M2 6 L2 30 30 30 30 6 Z M2 15 L30 15 M7 3 L7 9 M13 3 L13 9 M19 3 L19 9 M25 3 L25 9"></path>
  </svg>
  &nbsp;
  2022-10-20
</time>





    
    &nbsp;
    <svg class="i-tag" viewBox="0 0 32 32" width="16" height="16" fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2">
      <circle cx="24" cy="8" r="2"></circle>
      <path d="M2 18 L18 2 30 2 30 14 14 30 Z"></path>
    </svg>
    &nbsp;
    <a href="/tags/机器学习/">机器学习</a>


    </div>
    <hr>
    
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-text">线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95"><span class="toc-text">最小二乘法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%99%AA%E5%A3%B0%E4%B8%BA%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83%E7%9A%84-MLE"><span class="toc-text">噪声为高斯分布的 MLE</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9D%83%E9%87%8D%E5%85%88%E9%AA%8C%E4%B9%9F%E4%B8%BA%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83%E7%9A%84-MAP"><span class="toc-text">权重先验也为高斯分布的 MAP</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-text">正则化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#L1-Lasso"><span class="toc-text">L1 Lasso</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#L2-Ridge"><span class="toc-text">L2 Ridge</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%8F%E7%BB%93"><span class="toc-text">小结</span></a></li></ol></li></ol>
    
    <div class="picture-container">
      
    </div>
    <h1 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h1><p>假设数据集为：<br>$$<br>\mathcal{D}&#x3D;{(x_1, y_1),(x_2, y_2),\cdots,(x_N, y_N)}<br>$$<br>后面我们记：<br>$$<br>X&#x3D;(x_1,x_2,\cdots,x_N)^T,Y&#x3D;(y_1,y_2,\cdots,y_N)^T<br>$$<br>线性回归假设：<br>$$<br>f(w)&#x3D;w^Tx<br>$$</p>
<h2 id="最小二乘法"><a href="#最小二乘法" class="headerlink" title="最小二乘法"></a>最小二乘法</h2><p>对这个问题，采用二范数定义的平方误差来定义损失函数：<br>$$<br>L(w)&#x3D;\sum\limits_{i&#x3D;1}^N||w^Tx_i-y_i||^2_2<br>$$<br>展开得到：<br>$$<br>\begin{align}<br>L(w)&amp;&#x3D;(w^Tx_1-y_1,\cdots,w^Tx_N-y_N)\cdot (w^Tx_1-y_1,\cdots,w^Tx_N-y_N)^T\nonumber\<br>&amp;&#x3D;(w^TX^T-Y^T)\cdot (Xw-Y)&#x3D;w^TX^TXw-Y^TXw-w^TX^TY+Y^TY\nonumber\<br>&amp;&#x3D;w^TX^TXw-2w^TX^TY+Y^TY<br>\end{align}<br>$$<br>最小化这个值的 $ \hat{w}$ ：<br>$$<br>\begin{align}<br>\hat{w}&#x3D;\mathop{argmin}\limits_wL(w)&amp;\longrightarrow\frac{\partial}{\partial w}L(w)&#x3D;0\nonumber\<br>&amp;\longrightarrow2X^TX\hat{w}-2X^TY&#x3D;0\nonumber\<br>&amp;\longrightarrow \hat{w}&#x3D;(X^TX)^{-1}X^TY&#x3D;X^+Y<br>\end{align}<br>$$<br>这个式子中 $(X^TX)^{-1}X^T$ 又被称为伪逆。对于行满秩或者列满秩的 $X$，可以直接求解，但是对于非满秩的样本集合，需要使用奇异值分解（SVD）的方法，对 $X$ 求奇异值分解，得到<br>$$<br>X&#x3D;U\Sigma V^T<br>$$<br>于是：<br>$$<br>X^+&#x3D;V\Sigma^{-1}U^T<br>$$<br>在几何上，最小二乘法相当于模型（这里就是直线）和试验值的距离的平方求和，假设我们的试验样本张成一个 $p$ 维空间（满秩的情况）：$X&#x3D;Span(x_1,\cdots,x_N)$，而模型可以写成 $f(w)&#x3D;X\beta$，也就是 $x_1,\cdots,x_N$ 的某种组合，而最小二乘法就是说希望 $Y$ 和这个模型距离越小越好，于是它们的差应该与这个张成的空间垂直：<br>$$<br>X^T\cdot(Y-X\beta)&#x3D;0\longrightarrow\beta&#x3D;(X^TX)^{-1}X^TY<br>$$</p>
<h2 id="噪声为高斯分布的-MLE"><a href="#噪声为高斯分布的-MLE" class="headerlink" title="噪声为高斯分布的 MLE"></a>噪声为高斯分布的 MLE</h2><p>对于一维的情况，记 $y&#x3D;w^Tx+\epsilon,\epsilon\sim\mathcal{N}(0,\sigma^2)$，那么 $y\sim\mathcal{N}(w^Tx,\sigma^2)$。代入极大似然估计中：<br>$$<br>\begin{align}<br>L(w)&#x3D;\log p(Y|X,w)&amp;&#x3D;\log\prod\limits_{i&#x3D;1}^Np(y_i|x_i,w)\nonumber\<br>&amp;&#x3D;\sum\limits_{i&#x3D;1}^N\log(\frac{1}{\sqrt{2\pi\sigma}}e^{-\frac{(y_i-w^Tx_i)^2}{2\sigma^2}})\<br>\mathop{argmax}\limits_wL(w)&amp;&#x3D;\mathop{argmin}\limits_w\sum\limits_{i&#x3D;1^N}(y_i-w^Tx_i)^2<br>\end{align}<br>$$<br>这个表达式和最小二乘估计得到的结果一样。</p>
<h2 id="权重先验也为高斯分布的-MAP"><a href="#权重先验也为高斯分布的-MAP" class="headerlink" title="权重先验也为高斯分布的 MAP"></a>权重先验也为高斯分布的 MAP</h2><p>取先验分布 $w\sim\mathcal{N}(0,\sigma_0^2)$。于是： <br>$$<br>\begin{align}<br>\hat{w}&#x3D;\mathop{argmax}\limits_wp(w|Y)&amp;&#x3D;\mathop{argmax}\limits_wp(Y|w)p(w)\nonumber\<br>&amp;&#x3D;\mathop{argmax}\limits_w\log p(Y|w)p(w)\nonumber\<br>&amp;&#x3D;\mathop{argmax}\limits_w(\log p(Y|w)+\log p(w))\nonumber\<br>&amp;&#x3D;\mathop{argmin}\limits_w[(y-w^Tx)^2+\frac{\sigma^2}{\sigma_0^2}w^Tw]<br>\end{align}<br>$$<br>这里省略了 $X$，$p(Y)$和 $w$ 没有关系，同时也利用了上面高斯分布的 MLE的结果。</p>
<p>我们将会看到，超参数 $\sigma_0$的存在和下面会介绍的 Ridge 正则项可以对应，同样的如果将先验分布取为 Laplace 分布，那么就会得到和 L1 正则类似的结果。</p>
<h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><p>在实际应用时，如果样本容量不远远大于样本的特征维度，很可能造成过拟合，对这种情况，我们有下面三个解决方式：</p>
<ol>
<li>加数据</li>
<li>特征选择（降低特征维度）如 PCA 算法。</li>
<li>正则化</li>
</ol>
<p>正则化一般是在损失函数（如上面介绍的最小二乘损失）上加入正则化项（表示模型的复杂度对模型的惩罚），下面我们介绍一般情况下的两种正则化框架。<br>$$<br>\begin{align}<br>L1&amp;:\mathop{argmin}\limits_wL(w)+\lambda||w||_1,\lambda\gt0\<br>L2&amp;:\mathop{argmin}\limits_wL(w)+\lambda||w||^2_2,\lambda \gt 0<br>\end{align}<br>$$<br>下面对最小二乘误差分别分析这两者的区别。</p>
<h3 id="L1-Lasso"><a href="#L1-Lasso" class="headerlink" title="L1 Lasso"></a>L1 Lasso</h3><p>L1正则化可以引起稀疏解。</p>
<p>从最小化损失的角度看，由于 L1 项求导在0附近的左右导数都不是0，因此更容易取到0解。</p>
<p>从另一个方面看，L1 正则化相当于：<br>$$<br>\mathop{argmin}\limits_wL(w)\<br>s.t. ||w||_1\lt C<br>$$<br>我们已经看到平方误差损失函数在 $w$ 空间是一个椭球，因此上式求解就是椭球和 $||w||_1&#x3D;C$的切点，因此更容易相切在坐标轴上。</p>
<h3 id="L2-Ridge"><a href="#L2-Ridge" class="headerlink" title="L2 Ridge"></a>L2 Ridge</h3><p>$$<br>\begin{align}<br>\hat{w}&#x3D;\mathop{argmin}\limits_wL(w)+\lambda w^Tw&amp;\longrightarrow\frac{\partial}{\partial w}L(w)+2\lambda w&#x3D;0\nonumber\<br>&amp;\longrightarrow2X^TX\hat{w}-2X^TY+2\lambda \hat w&#x3D;0\nonumber\<br>&amp;\longrightarrow \hat{w}&#x3D;(X^TX+\lambda \mathbb{I})^{-1}X^TY<br>\end{align}<br>$$</p>
<p>可以看到，这个正则化参数和前面的 MAP 结果不谋而合。利用2范数进行正则化不仅可以是模型选择 $w$ 较小的参数，同时也避免 $ X^TX$不可逆的问题。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>线性回归模型是最简单的模型，但是麻雀虽小，五脏俱全，在这里，我们利用最小二乘误差得到了闭式解。同时也发现，在噪声为高斯分布的时候，MLE 的解等价于最小二乘误差，而增加了正则项后，最小二乘误差加上 L2 正则项等价于高斯噪声先验下的 MAP解，加上 L1 正则项后，等价于 Laplace 噪声先验。</p>
<p>传统的机器学习方法或多或少都有线性回归模型的影子：</p>
<ol>
<li>线性模型往往不能很好地拟合数据，因此有三种方案克服这一劣势：<ol>
<li>对特征的维数进行变换，例如多项式回归模型就是在线性特征的基础上加入高次项。</li>
<li>在线性方程后面加入一个非线性变换，即引入一个非线性的激活函数，典型的有线性分类模型如感知机。</li>
<li>对于一致的线性系数，我们进行多次变换，这样同一个特征不仅仅被单个系数影响，例如多层感知机（深度前馈网络）。</li>
</ol>
</li>
<li>线性回归在整个样本空间都是线性的，我们修改这个限制，在不同区域引入不同的线性或非线性，例如线性样条回归和决策树模型。</li>
<li>线性回归中使用了所有的样本，但是对数据预先进行加工学习的效果可能更好（所谓的维数灾难，高维度数据更难学习），例如 PCA 算法和流形学习。</li>
</ol>


  </article>
  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  <div class="busuanzi center">
    page PV:&nbsp;<span id="busuanzi_value_page_pv"></span>&nbsp;・&nbsp;
    site PV:&nbsp;<span id="busuanzi_value_site_pv"></span>&nbsp;・&nbsp;
    site UV:&nbsp;<span id="busuanzi_value_site_uv"></span>
  </div>


    





    </div>
  </div>
  <footer class="page-footer"><div class="clearfix">
</div>
<div class="right-foot">
    <div class="firstrow">
        <a href="#top" target="_self">
        <svg class="i-caret-right" viewBox="0 0 32 32" width="24" height="24" fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="3">
            <path d="M10 30 L26 16 10 2 Z"></path>
        </svg>
        </a>
        © peirsist 2022
    </div>
    <div class="secondrow">
        <a target="_blank" rel="noopener" href="https://github.com/gaoryrt/hexo-theme-pln">
        Theme Pln
        </a>
    </div>
</div>
<div class="clearfix">
</div>
</footer>
  <script src="//cdn.bootcss.com/jquery/2.2.1/jquery.min.js"></script>
<script src="/js/search.min.js"></script>
<script type="text/javascript">

// disqus scripts


// dropdown scripts
$(".dropdown").click(function(event) {
  var current = $(this);
  event.stopPropagation();
  $(current).children(".dropdown-content")[($(current).children(".dropdown-content").hasClass("open"))?'removeClass':'addClass']("open")
});
$(document).click(function(){
    $(".dropdown-content").removeClass("open");
})

var path = "/search.xml";
searchFunc(path, 'local-search-input', 'local-search-result');

</script>

</body>
</html>
